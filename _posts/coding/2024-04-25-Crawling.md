---
layout: post
title: python) Crawling 기본
date: 2024-04-25 20:00 +0900
description: 
image: https://github.com/Kingsong97/Kingsong97.github.io/assets/161429740/db65de14-dc59-4576-8cec-82acc61d8013

category: Python
tags: Crawling Python
published: true
sitemap: true
---
안녕하세요, 여러분! 오늘은 Python을 사용하여 웹 크롤링하는 기본적인 방법을 알아볼까 합니다. 웹 크롤링은 웹 페이지에서 데이터를 수집하는 자동화된 방법으로, 다양한 데이터 분석 프로젝트나 정보 수집에 매우 유용합니다.

## 1. 필요한 라이브러리 설치하기
웹 크롤링을 위해 Python에서 가장 많이 사용되는 라이브러리는 requests와 BeautifulSoup4입니다. 이 두 라이브러리를 사용하면 웹 사이트의 HTML을 쉽게 요청하고 파싱할 수 있습니다. 먼저, 이 라이브러리들을 설치해야 합니다. 
> beautifulsoup4

## 2. 웹 페이지 가져오기
requests 라이브러리를 사용하여 웹 페이지의 HTML을 가져올 수 있습니다. 예를 들어, 특정 웹페이지의 정보를 가져오고 싶다면 다음과 같이 코드를 작성합니다.

```python
import requests

url = 'http://example.com'
response = requests.get(url)
html_content = response.text
```
## 3. HTML 파싱하기
가져온 HTML을 분석하기 위해 BeautifulSoup 객체를 생성합니다. 이 객체를 통해 HTML 내의 특정 요소를 쉽게 찾을 수 있습니다. 다음은 BeautifulSoup을 사용하는 방법입니다.

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html_content, 'html.parser')
```

## 4. 원하는 데이터 추출하기
이제 BeautifulSoup을 사용하여 원하는 데이터를 추출할 수 있습니다. 예를 들어, 모든 <a> 태그를 찾고 싶다면 다음과 같이 코드를 작성할 수 있습니다.

```python
links = soup.find_all('#')
for link in links:
    print(link.get('href'))
```

## 5. 5. 데이터 정리 및 저장
추출한 데이터를 파일로 저장하거나 다른 형태로 정리할 수 있습니다. 예를 들어, 모든 링크를 텍스트 파일에 저장하려면 다음과 같이 할 수 있습니다:

```python
with open('links.txt', 'w') as file:
    for link in links:
        file.write(link.get('href') + '\n')
```

이렇게 간단한 웹 크롤링을 통해 웹 페이지에서 유용한 데이터를 수집하고, 이를 다양한 형태로 저장하여 활용할 수 있습니다. 웹 크롤링은 매우 강력한 도구이지만, 사용할 때는 항상 웹사이트의 robots.txt를 확인하고, 사이트의 이용 약관을 준수해야 합니다. 웹사이트에 과도한 부하를 주지 않도록 주의하며, 데이터 수집을 책임감 있게 진행하시길 바랍니다. Happy Crawling!
